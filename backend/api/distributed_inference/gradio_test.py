# import gradio as gr
# import torch
# from transformers import AutoModelForCausalLM, AutoTokenizer
# from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer
# from threading import Thread

# # Loading the tokenizer and model from Hugging Face's model hub.
# tokenizer = AutoTokenizer.from_pretrained(
#     "TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T"
# )
# model = AutoModelForCausalLM.from_pretrained(
#     "TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T"
# )

# # using CUDA for an optimal experience
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)


# # Defining a custom stopping criteria class for the model's text generation.
# class StopOnTokens(StoppingCriteria):
#     def __call__(
#         self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
#     ) -> bool:
#         stop_ids = [2]  # IDs of tokens where the generation should stop.
#         for stop_id in stop_ids:
#             if (
#                 input_ids[0][-1] == stop_id
#             ):  # Checking if the last generated token is a stop token.
#                 return True
#         return False


# # Function to generate model predictions.
# def predict(message, history):
#     history_transformer_format = history + [[message, ""]]
#     stop = StopOnTokens()

#     # Formatting the input for the model.
#     messages = "</s>".join(
#         [
#             "</s>".join(["\n<|user|>:" + item[0], "\n<|assistant|>:" + item[1]])
#             for item in history_transformer_format
#         ]
#     )
#     model_inputs = tokenizer([messages], return_tensors="pt").to(device)
#     streamer = TextIteratorStreamer(
#         tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True
#     )
#     generate_kwargs = dict(
#         model_inputs,
#         streamer=streamer,
#         max_new_tokens=1024,
#         do_sample=True,
#         top_p=0.95,
#         top_k=50,
#         temperature=1,
#         num_beams=1,
#         stopping_criteria=StoppingCriteriaList([stop]),
#     )
#     t = Thread(target=model.generate, kwargs=generate_kwargs)
#     t.start()  # Starting the generation in a separate thread.
#     partial_message = ""
#     for new_token in streamer:
#         partial_message += new_token
#         if (
#             "</s>" in partial_message
#         ):  # Breaking the loop if the stop token is generated.
#             break
#         yield partial_message


# # Setting up the Gradio chat interface.
# interface = gr.ChatInterface(
#     predict,
#     title="Tinyllama_chatBot",
#     description="Ask Tiny llama any questions",
#     examples=["How to cook a fish?", "Who is the president of US now?"],
# )
# interface.queue()  # Enable the queue for the interface using the .queue() method.
# interface.launch()  # Launching the web interface.
